{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# ChromeDriverManager를 통해 드라이버 경로를 설정\n",
    "service = Service(ChromeDriverManager().install())\n",
    "\n",
    "# 웹 드라이버 초기화\n",
    "driver = webdriver.Chrome(service=service)\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 채용 공고 가져오기\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Selenium WebDriver 설정\n",
    "driver = webdriver.Chrome() \n",
    "\n",
    "# 결과를 저장할 리스트\n",
    "company_name = [] # 회사명\n",
    "job_posting_title = [] # 공고 제목\n",
    "work_place = [] # 기업 위치\n",
    "education = [] # 학력\n",
    "career = [] # 경력\n",
    "dead_line = [] # 마감일\n",
    "skill = [] # 기술 스택\n",
    "\n",
    "# 채용 공고를 낸 회사 정보 저장할 리스트\n",
    "company_place = [] # 회사 위치\n",
    "company_category = [] # 업종\n",
    "company_url = [] # 홈페이지지\n",
    "\n",
    "\n",
    "\n",
    "# 날짜 데이터 전처리\n",
    "def convert_deadline(deadline_text):\n",
    "    try:\n",
    "        # D-_일 형태\n",
    "        if 'D-' in deadline_text:\n",
    "            days_remaining = int(deadline_text.split('D-')[1].split('일')[0])\n",
    "            return (datetime.now() + timedelta(days=days_remaining)).strftime('%Y.%m.%d')\n",
    "\n",
    "        # ~__.__(요일) 형태\n",
    "        elif '~' in deadline_text:\n",
    "            date_str = deadline_text.split('~')[1].split('(')[0].strip()  \n",
    "            date_obj = datetime.strptime(date_str, '%m.%d')  \n",
    "            return date_obj.replace(year=datetime.now().year).strftime('%Y.%m.%d')\n",
    "\n",
    "        # 내일마감\n",
    "        elif '내일마감' in deadline_text:\n",
    "            return (datetime.now() + timedelta(days=1)).strftime('%Y.%m.%d')\n",
    "\n",
    "        # 채용시\n",
    "        elif '채용시' in deadline_text:\n",
    "            return '채용시'\n",
    "\n",
    "        return deadline_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"마감일 변환 중 오류 발생: {e}\")\n",
    "        return deadline_text  # 오류가 발생하면 원본 텍스트 반환\n",
    "\n",
    "\n",
    "# 페이지 넘기면서\n",
    "for i in range(1, 2):\n",
    "    try:\n",
    "\n",
    "        # 페이지 URL\n",
    "        url = f\"https://www.saramin.co.kr/zf_user/jobs/list/job-category?page={i}&cat_mcls=2&search_optional_item=n&search_done=y&panel_count=y&preview=y&isAjaxRequest=0&page_count=50&sort=RL&type=job-category&is_param=1&isSearchResultEmpty=1&isSectionHome=0&searchParamCount=1#searchTitle\"\n",
    "        driver.get(url)  # 페이지 열기\n",
    "        time.sleep(2)  # 벤 안먹기 위한 슬립\n",
    "\n",
    "        # 페이지 소스를 BeautifulSoup으로 파싱\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # 회사 이름이 하이퍼 링크인 경우만 회사 정보 & 채용 공고 가져오기\n",
    "        # 회사 이름과 채용 공고 제목 추출\n",
    "        atagData = soup.find_all(['a'], class_='str_tit')  # 'a'와 'span' 태그에서 class='str_tit'을 가진 모든 요소 찾기\n",
    "\n",
    "        for company in atagData:\n",
    "\n",
    "\n",
    "            # 'a' 태그인 경우 href 속성을 통해 회사 이름과 채용 공고 제목을 구분\n",
    "            if company.name == 'a' and 'company-info' in company.get('href', ''):  # 회사 이름 조건\n",
    "                company_href = company.get('href')\n",
    "                href = \"https://www.saramin.co.kr\" + company_href\n",
    "\n",
    "                driver.get(href)\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "                base_url = 'https://www.saramin.co.kr/zf_user/company-info/view?csn='\n",
    "                selected_company = driver.current_url\n",
    "                tmp = selected_company[70:]\n",
    "                time.sleep(2)\n",
    "                \n",
    "                # 기업소개 버튼\n",
    "                move_to_company_descript= base_url + tmp\n",
    "\n",
    "                driver.get(move_to_company_descript)\n",
    "\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                time.sleep(2)\n",
    "                # 기업 데이터 가져오기\n",
    "                company_data = soup.select(\"dl.company_details > div.company_details_group > dd.desc\")\n",
    "\n",
    "                # 회사 데이터 추출\n",
    "                if len(company_data) > 0:\n",
    "                    # 1번째 데이터 (업종 정보)\n",
    "                    company_category.append(company_data[0].get_text(strip=True))\n",
    "                    \n",
    "                else:\n",
    "                    company_category.append(\"nodata\")\n",
    "\n",
    "                if len(company_data) > 2:\n",
    "                    # 3번째 데이터 (홈페이지 URL)\n",
    "                    homepage_tag = company_data[2].find(\"a\", target=\"_blank\")\n",
    "                    company_url.append(homepage_tag[\"href\"] if homepage_tag else \"No homepage URL\")\n",
    "                else:\n",
    "                    None\n",
    "\n",
    "                if len(company_data) > 4:\n",
    "                    # 5번째 데이터 (주소 정보)\n",
    "                    address_tag = company_data[4].find(\"p\", class_=\"ellipsis\")\n",
    "                    company_place.append(address_tag.get_text(strip=True) if address_tag else \"No address\") \n",
    "                else:\n",
    "                    None\n",
    "\n",
    "                time.sleep(2)\n",
    "\n",
    "                company_name.append(company.get_text(strip=True))\n",
    "            if company.name == 'a' and 'jobs/relay' in company.get('href', ''):  # 채용 공고 제목 조건\n",
    "                job_posting_title.append(company.get_text(strip=True))\n",
    "        \n",
    "            # 'span' 태그인 경우에도 텍스트 추출\n",
    "            if company.name == 'span':  # 'span' 태그는 href 속성이 없으므로 그냥 텍스트 추출\n",
    "                company_name.append(company.get_text(strip=True))  # 'span' 안의 텍스트를 회사 이름으로 간주\n",
    "        \n",
    "        # 기술 스택 추출\n",
    "        sData = soup.find_all('div', class_='job_meta')\n",
    "        for skilldata in sData:\n",
    "            # job_sector 클래스 내부의 span 태그 추출\n",
    "            sector_spans = skilldata.find_all('span', class_='job_sector')\n",
    "            # 5개씩 한 묶음\n",
    "            single_skill_set = [] \n",
    "            for sector in sector_spans:\n",
    "                # 클래스가 없는 내부 span 태그 추출\n",
    "                inner_spans = sector.find_all('span', class_=None)\n",
    "                for span in inner_spans:\n",
    "                    single_skill_set.append(span.get_text(strip=True))\n",
    "\n",
    "            if single_skill_set:\n",
    "                skill.append(\",\".join(single_skill_set))        \n",
    "\n",
    "        # work_place, education, career, postdate, deadline 추출\n",
    "        work_place_tags = soup.find_all('p', class_='work_place')\n",
    "        education_tags = soup.find_all('p', class_='education')\n",
    "        career_tags = soup.find_all('p', class_='career')\n",
    "\n",
    "        deadline_tags = soup.find_all('p', class_='support_detail')\n",
    "        for tag in deadline_tags:\n",
    "            span_tag = tag.find('span', class_='date')\n",
    "            if span_tag:\n",
    "                dead_line.append(convert_deadline(span_tag.get_text(strip=True))) # 마감일 정규화\n",
    "\n",
    "\n",
    "        # 저장\n",
    "        for tag in work_place_tags:\n",
    "            work_place.append(tag.get_text(strip=True))\n",
    "        for tag in education_tags:\n",
    "            education.append(tag.get_text(strip=True))\n",
    "        for tag in career_tags:\n",
    "            career.append(tag.get_text(strip=True))\n",
    "\n",
    "    except Exception as e:\n",
    "         print(f\"에러 발생: {e}\")\n",
    "\n",
    "#driver.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기업 정보 가져오기\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# WebDriver 설정\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# 결과 저장 리스트\n",
    "curation_company_name = []  # 기업명\n",
    "curation_company_type = []  # 기업 형태\n",
    "curation_company_year = []  # 기업 설립 연도\n",
    "curation_company_gerne = []  # 기업 업종\n",
    "\n",
    "# 초기 URL 설정\n",
    "url = 'https://www.saramin.co.kr/zf_user/company-info/sri-certification?seq=236'\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "# 페이지 버튼 누르면서 반복\n",
    "for page in range(1, 7): \n",
    "    try:\n",
    "        # 페이지 소스 가져오기\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # 큐레이션 리스트에 있는 li 탐색\n",
    "        ul_tags = soup.findAll('ul', class_='list_employ list_company')\n",
    "\n",
    "        for ul_tag in ul_tags:\n",
    "            # 회사명 태그 찾기\n",
    "            name_tags = ul_tag.find_all('a', class_='tit')\n",
    "            for name_tag in name_tags:\n",
    "                curation_company_name.append(name_tag.get_text(strip=True))\n",
    "\n",
    "            # 회사 설명 태그 찾기    \n",
    "            state_tags = ul_tag.find_all('div', class_='state')\n",
    "            for state_tag in state_tags:\n",
    "                spans = state_tag.find_all('span')\n",
    "                if len(spans) >= 3:\n",
    "                    # 각 항목이 비어 있으면 '입력되지 않음' 추가\n",
    "                    curation_company_type.append(spans[0].get_text(strip=True) if spans[0].get_text(strip=True) else \"입력되지 않음\")\n",
    "                    curation_company_year.append(spans[1].get_text(strip=True) if spans[1].get_text(strip=True) else \"입력되지 않음\")\n",
    "                    curation_company_gerne.append(spans[2].get_text(strip=True) if spans[2].get_text(strip=True) else \"입력되지 않음\")\n",
    "                else:\n",
    "                    # span 태그가 부족할 경우 '입력되지 않음' 추가\n",
    "                    curation_company_type.append(\"입력되지 않음\")\n",
    "                    curation_company_year.append(\"입력되지 않음\")\n",
    "                    curation_company_gerne.append(\"입력되지 않음\")\n",
    "\n",
    "        # 다음 페이지 버튼 클릭\n",
    "        page_link = driver.find_element(By.CSS_SELECTOR, f'a.page[data-page=\"{page + 1}\"]')\n",
    "        page_link.click()\n",
    "        time.sleep(2)  # 페이지 이동 후 대기\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"페이지 {page} 처리 중 에러 발생: {e}\")\n",
    "        break\n",
    "\n",
    "#driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from itertools import zip_longest\n",
    "\n",
    "# 데이터 예시 (리스트가 동일한 길이라고 가정)\n",
    "\n",
    "posting_company_data = {\n",
    "    \"company_category\" : company_category,\n",
    "    \"company_place\" : company_place,\n",
    "    \"company_url\" : company_url,\n",
    "}\n",
    "\n",
    "posting_data = {\n",
    "    \"company_name\": company_name,\n",
    "    \"job_posting_title\": job_posting_title,\n",
    "    \"work_place\": work_place,\n",
    "    \"education\": education,\n",
    "    \"career\": career,\n",
    "    \"dead_line\": dead_line,\n",
    "    \"skill\": skill,\n",
    "    \n",
    "}\n",
    "\n",
    "curation_company_data = {\n",
    "    \"curation_company_name\": curation_company_name,\n",
    "    \"curation_company_type\": curation_company_type,\n",
    "    \"curation_company_year\": curation_company_year,\n",
    "    \"curation_company_gerne\": curation_company_gerne,\n",
    "}\n",
    "\n",
    "\n",
    "# CSV 파일 저장\n",
    "with open('data.csv', 'w', encoding='utf-8', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    # CSV 파일의 헤더 작성\n",
    "    writer.writerow(posting_data.keys())\n",
    "    \n",
    "\n",
    "    # 데이터를 행 단위로 작성 (zip 사용)\n",
    "    p_rows = zip_longest(*posting_data.values(), fillvalue='')\n",
    "    writer.writerows(p_rows)\n",
    "    \n",
    "with open('company_data.csv','w',encoding='utf-8', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    writer.writerow(curation_company_data.keys())\n",
    "\n",
    "    c_rows = zip_longest(*curation_company_data.values(), fillvalue='')\n",
    "    writer.writerows(c_rows)\n",
    "\n",
    "\n",
    "with open('posting_company_data', 'w', encoding='utf-8', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    writer.writerow(posting_company_data.keys())\n",
    "\n",
    "    b_row = zip_longest(*posting_company_data.values(), fillvalue='')\n",
    "    writer.writerows(b_row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB 연결 성공!\n",
      "DB 연결 닫힘.\n"
     ]
    }
   ],
   "source": [
    "# DB 값 저장하기\n",
    "import pymysql\n",
    "\n",
    "# DB 정보\n",
    "host = \"localhost\"\n",
    "user = \"root\"\n",
    "password = \"1234\"\n",
    "database = \"job_posting\"\n",
    "\n",
    "# DB 연결된 상태인지 확인하기 위한 try\n",
    "try:\n",
    "    conn = pymysql.connect(host=host, user=user, password=password, db=database)\n",
    "    print(\"DB 연결 성공!\")\n",
    "    curs = conn.cursor(pymysql.cursors.DictCursor)\n",
    "\n",
    "\n",
    "\n",
    "except pymysql.MySQLError as e:\n",
    "    print(f\"DB 연결 실패: {e}\")\n",
    "finally:\n",
    "    if 'conn' in locals() and conn.open:\n",
    "        conn.close()\n",
    "        print(\"DB 연결 닫힘.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    }
   ],
   "source": [
    "str = 'https://www.saramin.co.kr/zf_user/company-info/view?csn='\n",
    "\n",
    "print(len(str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

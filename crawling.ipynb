{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "service = Service(ChromeDriverManager().install())\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ChromeDriverManager를 통해 드라이버 경로를 설정\n",
    "\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 채용 공고 가져오기\n",
    "\n",
    "# Selenium WebDriver 설정\n",
    "driver = webdriver.Chrome() \n",
    "\n",
    "# 결과를 저장할 리스트\n",
    "company_name = [] # 회사명\n",
    "title = [] # 공고 제목\n",
    "education = [] # 학력\n",
    "career = [] # 경력\n",
    "dead_line = [] # 마감일\n",
    "skill = [] # 기술 스택\n",
    "\n",
    "# 채용 공고를 낸 회사 정보 저장할 리스트\n",
    "company_place = [] # 회사 위치\n",
    "company_category = [] # 업종\n",
    "company_url = [] # 홈페이지\n",
    "\n",
    "\n",
    "\n",
    "# 날짜 데이터 전처리\n",
    "def convert_deadline(deadline_text):\n",
    "    try:\n",
    "        # D-_일 형태\n",
    "        if 'D-' in deadline_text:\n",
    "            days_remaining = int(deadline_text.split('D-')[1].split('일')[0])\n",
    "            return (datetime.now() + timedelta(days=days_remaining)).strftime('%Y.%m.%d')\n",
    "\n",
    "        # ~__.__(요일) 형태\n",
    "        elif '~' in deadline_text:\n",
    "            date_str = deadline_text.split('~')[1].split('(')[0].strip()  \n",
    "            date_obj = datetime.strptime(date_str, '%m.%d')  \n",
    "            return date_obj.replace(year=datetime.now().year).strftime('%Y.%m.%d')\n",
    "\n",
    "        # 내일마감\n",
    "        elif '내일마감' in deadline_text:\n",
    "            return (datetime.now() + timedelta(days=1)).strftime('%Y.%m.%d')\n",
    "\n",
    "        # 채용시\n",
    "        elif '채용시' in deadline_text:\n",
    "            return None\n",
    "        \n",
    "        elif '상시채용' in deadline_text:\n",
    "            return None\n",
    "\n",
    "        return deadline_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"마감일 변환 중 오류 발생: {e}\")\n",
    "        return deadline_text  # 오류가 발생하면 원본 텍스트 반환\n",
    "\n",
    "\n",
    "# 페이지 넘기면서\n",
    "for i in range(1, 4):\n",
    "    try:\n",
    "\n",
    "        # 페이지 URL\n",
    "        url = f\"https://www.saramin.co.kr/zf_user/jobs/list/job-category?page={i}&cat_mcls=2&search_optional_item=n&search_done=y&panel_count=y&preview=y&isAjaxRequest=0&page_count=50&sort=RL&type=job-category&is_param=1&isSearchResultEmpty=1&isSectionHome=0&searchParamCount=1#searchTitle\"\n",
    "        driver.get(url)  # 페이지 열기\n",
    "        time.sleep(2)  # 벤 안먹기 위한 슬립\n",
    "        current_url = driver.current_url\n",
    "\n",
    "        # 페이지 소스를 BeautifulSoup으로 파싱\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # 회사 이름이 하이퍼 링크인 경우만 회사 정보 & 채용 공고 가져오기\n",
    "        # 회사 이름과 채용 공고 제목 추출\n",
    "        atagData = soup.find_all(['a'], class_='str_tit')  # 'a'와 'span' 태그에서 class='str_tit'을 가진 모든 요소 찾기\n",
    "\n",
    "        for company in atagData:\n",
    "    \n",
    "            if company.name == 'a' and 'jobs/relay' in company.get('href', ''):\n",
    "                title.append(company.get_text(strip=True))\n",
    "    \n",
    "            # 'a' 태그인 경우 href 속성을 통해 회사 이름과 채용 공고 제목을 구분\n",
    "            if company.name == 'a' and 'company-info' in company.get('href', ''):  \n",
    "                # 회사 이름 조건                               # 채용 공고 제목 조건\n",
    "\n",
    "                \n",
    "                company_name.append(company.get_text(strip=True))\n",
    "                company_href = company.get('href')\n",
    "                href = \"https://www.saramin.co.kr\" + company_href\n",
    "\n",
    "                driver.get(href)\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "                base_url = 'https://www.saramin.co.kr/zf_user/company-info/view?csn='\n",
    "                selected_company = driver.current_url\n",
    "                tmp = selected_company[70:]\n",
    "                time.sleep(2)\n",
    "                \n",
    "                # 기업소개 버튼\n",
    "                move_to_company_descript= base_url + tmp\n",
    "\n",
    "                # \n",
    "                driver.get(move_to_company_descript)\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                time.sleep(2)\n",
    "\n",
    "                # 기업 데이터 가져오기\n",
    "                company_data = soup.select(\"dl.company_details > div.company_details_group > dd.desc\")\n",
    "\n",
    "                # 회사 데이터 추출\n",
    "                if len(company_data) > 0:\n",
    "                    # 1번째 데이터 (업종 정보)\n",
    "                    company_category.append(company_data[0].get_text(strip=True) if company_category else \"No category\")\n",
    "                    \n",
    "                else:\n",
    "                    company_category.append(\"No category\")\n",
    "\n",
    "                if len(company_data) > 2:\n",
    "                    # 3번째 데이터 (홈페이지 URL)\n",
    "                    homepage_tag = company_data[2].find(\"a\", target=\"_blank\")\n",
    "                    company_url.append(homepage_tag[\"href\"] if homepage_tag else \"No homepage URL\")\n",
    "                else:\n",
    "                    company_url.append(\"No hompage URL\")\n",
    "\n",
    "                if len(company_data) > 4:\n",
    "                    # 5번째 데이터 (주소 정보)\n",
    "                    address_tag = company_data[4].find(\"p\", class_=\"ellipsis\")\n",
    "                    company_place.append(address_tag.get_text(strip=True) if address_tag else \"No address\") \n",
    "                else:\n",
    "                    company_place.append(\"No address\")\n",
    "\n",
    "                     \n",
    "                \n",
    "            time.sleep(2)   \n",
    "\n",
    "        driver.get(current_url)\n",
    "        time.sleep(1)\n",
    "        # 다시 읽어오기    \n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        time.sleep(2)\n",
    "        \n",
    "\n",
    "    \n",
    "        # 'span' 태그인 경우에도 텍스트 추출\n",
    "        if company.name == 'span':  \n",
    "            continue # 하이퍼 링크가 아니면 그냥 데이터를 추출하지 않음\n",
    "    \n",
    "        # 기술 스택 추출\n",
    "        sData = soup.find_all('div', class_='job_meta')\n",
    "        \n",
    "        for skilldata in sData:\n",
    "            # job_sector 클래스 내부의 span 태그 추출\n",
    "            sector_spans = skilldata.find_all('span', class_='job_sector')\n",
    "\n",
    "            # 5개씩 한 묶음\n",
    "            single_skill_set = [] \n",
    "            for sector in sector_spans:\n",
    "                # 클래스가 없는 내부 span 태그 추출\n",
    "                inner_spans = sector.find_all('span', class_=None)\n",
    "                for span in inner_spans:\n",
    "                    single_skill_set.append(span.get_text(strip=True))\n",
    "\n",
    "            if single_skill_set:\n",
    "                skill.append(\",\".join(single_skill_set))        \n",
    "\n",
    "        # education, career, postdate, deadline 추출\n",
    "        education_tags = soup.find_all('p', class_='education')\n",
    "        career_tags = soup.find_all('p', class_='career')\n",
    "        deadline_tags = soup.find_all('p', class_='support_detail')\n",
    "\n",
    "        for tag in deadline_tags:\n",
    "            span_tag = tag.find('span', class_='date')\n",
    "            if span_tag:\n",
    "                dead_line.append(convert_deadline(span_tag.get_text(strip=True)) if span_tag else \"마감 일 입력되지 않음\") # 마감일 정규화\n",
    "\n",
    "        # 저장\n",
    "        for tag in education_tags:\n",
    "            education.append(tag.get_text(strip=True) if tag else \"학력 사항 입력되지 않음\")\n",
    "        for tag in career_tags:\n",
    "            career.append(tag.get_text(strip=True) if tag else \"경력 사항 입력되지 않음\")\n",
    "\n",
    "    except Exception as e:\n",
    "         print(f\"에러 발생: {e}\")\n",
    "\n",
    "#driver.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기업 정보 가져오기\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# WebDriver 설정\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# 결과 저장 리스트\n",
    "curation_company_name = []  # 기업명\n",
    "curation_company_type = []  # 기업 형태\n",
    "curation_company_year = []  # 기업 설립 연도\n",
    "curation_company_genre = []  # 기업 업종\n",
    "\n",
    "# 초기 URL 설정\n",
    "url = 'https://www.saramin.co.kr/zf_user/company-info/sri-certification?seq=236'\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "# 페이지 버튼 누르면서 반복\n",
    "for page in range(1, 6): \n",
    "    try:\n",
    "        # 페이지 소스 가져오기\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # 큐레이션 리스트에 있는 li 탐색\n",
    "        ul_tags = soup.findAll('ul', class_='list_employ list_company')\n",
    "\n",
    "        for ul_tag in ul_tags:\n",
    "            # 회사명 태그 찾기\n",
    "            name_tags = ul_tag.find_all('a', class_='tit')\n",
    "            for name_tag in name_tags:\n",
    "                curation_company_name.append(name_tag.get_text(strip=True))\n",
    "\n",
    "            # 회사 설명 태그 찾기    \n",
    "            state_tags = ul_tag.find_all('div', class_='state')\n",
    "            for state_tag in state_tags:\n",
    "                spans = state_tag.find_all('span')\n",
    "                if len(spans) >= 3:\n",
    "                    # 각 항목이 비어 있으면 '입력되지 않음' 추가\n",
    "                    curation_company_type.append(spans[0].get_text(strip=True) if spans[0].get_text(strip=True) else \"입력되지 않음\")\n",
    "                    curation_company_year.append(spans[1].get_text(strip=True) if spans[1].get_text(strip=True) else \"입력되지 않음\")\n",
    "                    curation_company_genre.append(spans[2].get_text(strip=True) if spans[2].get_text(strip=True) else \"입력되지 않음\")\n",
    "                else:\n",
    "                    # span 태그가 부족할 경우 '입력되지 않음' 추가\n",
    "                    curation_company_type.append(\"입력되지 않음\")\n",
    "                    curation_company_year.append(\"입력되지 않음\")\n",
    "                    curation_company_genre.append(\"입력되지 않음\")\n",
    "\n",
    "        # 다음 페이지 버튼 클릭\n",
    "        page_link = driver.find_element(By.CSS_SELECTOR, f'a.page[data-page=\"{page + 1}\"]')\n",
    "        page_link.click()\n",
    "        time.sleep(2)  # 페이지 이동 후 대기\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"페이지 {page} 처리 중 에러 발생: {e}\")\n",
    "        break\n",
    "\n",
    "#driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from itertools import zip_longest\n",
    "\n",
    "# 데이터 예시 (리스트가 동일한 길이라고 가정)\n",
    "\n",
    "posting_company_data = {\n",
    "    \"company_name\" : company_name,\n",
    "    \"company_category\" : company_category,\n",
    "    \"company_place\" : company_place,\n",
    "    \"company_url\" : company_url,\n",
    "}\n",
    "\n",
    "posting_data = {\n",
    "    \"company_name\": company_name,\n",
    "    \"title\": title,\n",
    "    \"education\": education,\n",
    "    \"career\": career,\n",
    "    \"deadline\": dead_line,\n",
    "    \"skill\": skill,\n",
    "    \n",
    "}\n",
    "\n",
    "curation_company_data = {\n",
    "    \"curation_company_name\": curation_company_name,\n",
    "    \"curation_company_type\": curation_company_type,\n",
    "    \"curation_company_year\": curation_company_year,\n",
    "    \"curation_company_genre\": curation_company_genre,\n",
    "}\n",
    "\n",
    "\n",
    "# CSV 파일 저장\n",
    "with open('data.csv', 'w', encoding='utf-8', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    # CSV 파일의 헤더 작성\n",
    "    writer.writerow(posting_data.keys())\n",
    "    \n",
    "\n",
    "    # 데이터를 행 단위로 작성 (zip 사용)\n",
    "    p_rows = zip_longest(*posting_data.values(), fillvalue='')\n",
    "    writer.writerows(p_rows)\n",
    "    \n",
    "with open('company_data.csv','w',encoding='utf-8', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    writer.writerow(curation_company_data.keys())\n",
    "\n",
    "    c_rows = zip_longest(*curation_company_data.values(), fillvalue='')\n",
    "    writer.writerows(c_rows)\n",
    "\n",
    "\n",
    "with open('posting_company_data.csv', 'w', encoding='utf-8', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    writer.writerow(posting_company_data.keys())\n",
    "\n",
    "    b_row = zip_longest(*posting_company_data.values(), fillvalue='')\n",
    "    writer.writerows(b_row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB 연결 성공!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. 오류 발생: (1406, \"Data too long for column 'company_url' at row 1\")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2. 오류 발생 nan can not be used with MySQL\n",
      "회사를 찾을 수 없음: (주)영신디엔씨\n",
      "2. 오류 발생 nan can not be used with MySQL\n",
      "2. 오류 발생 nan can not be used with MySQL\n",
      "2. 오류 발생 nan can not be used with MySQL\n",
      "2. 오류 발생 nan can not be used with MySQL\n",
      "2. 오류 발생 nan can not be used with MySQL\n",
      "2. 오류 발생 nan can not be used with MySQL\n",
      "2. 오류 발생 nan can not be used with MySQL\n",
      "2. 오류 발생 nan can not be used with MySQL\n",
      "2. 오류 발생 nan can not be used with MySQL\n",
      "2. 오류 발생 nan can not be used with MySQL\n",
      "2. 오류 발생 nan can not be used with MySQL\n",
      "2. 오류 발생 nan can not be used with MySQL\n",
      "2. 오류 발생 nan can not be used with MySQL\n",
      "2. 오류 발생 nan can not be used with MySQL\n",
      "2. 오류 발생 nan can not be used with MySQL\n",
      "2. 오류 발생 nan can not be used with MySQL\n",
      "2. 오류 발생 nan can not be used with MySQL\n",
      "2. 오류 발생 nan can not be used with MySQL\n",
      "2. 오류 발생 nan can not be used with MySQL\n",
      "DB 연결 닫힘.\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "\n",
    "# DB 정보\n",
    "host = \"localhost\"\n",
    "user = \"root\"\n",
    "password = \"1234\"\n",
    "database = \"job_posting\"\n",
    "\n",
    "pos_com_data = pd.read_csv('posting_company_data.csv')\n",
    "pos_data = pd.read_csv('data.csv')\n",
    "curation_data = pd.read_csv('company_data.csv')\n",
    "\n",
    "# DB 연결된 상태인지 확인하기 위한 try\n",
    "try:\n",
    "    conn = pymysql.connect(host=host, user=user, password=password, db=database)\n",
    "    print(\"DB 연결 성공!\")\n",
    "    curs = conn.cursor(pymysql.cursors.DictCursor)\n",
    "\n",
    "    # 공고 회사 데이터 삽입\n",
    "    for index, row in pos_com_data.iterrows():\n",
    "        try:\n",
    "            # 중복 확인\n",
    "            check_sql = \"\"\"\n",
    "            SELECT COUNT(*) FROM Company WHERE company_name = %s\n",
    "            \"\"\"\n",
    "            curs.execute(check_sql, (row['company_name'],))\n",
    "            result = curs.fetchone()\n",
    "            if result['COUNT(*)'] == 0:  # 중복이 없을 경우\n",
    "                sql = \"\"\"\n",
    "                INSERT INTO Company (company_name, company_category, company_place, company_url)\n",
    "                VALUES (%s, %s, %s, %s)\n",
    "                \"\"\"\n",
    "                values = (\n",
    "                    row['company_name'],\n",
    "                    row['company_category'],\n",
    "                    row['company_place'],\n",
    "                    row['company_url']\n",
    "                )\n",
    "                curs.execute(sql, values)\n",
    "                conn.commit()\n",
    "            else:\n",
    "                print()#f\"중복된 회사명: {row['company_name']}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"1. 오류 발생: {e}\")\n",
    "            conn.rollback()\n",
    "\n",
    "    # 공고 정보 삽입 부분 수정\n",
    "    for index, row in pos_data.iterrows():\n",
    "        try:\n",
    "            # 회사명을 기반으로 company_id 조회\n",
    "            find_name_sql = \"\"\"\n",
    "            SELECT company_id FROM Company WHERE company_name = %s\n",
    "            \"\"\"\n",
    "            curs.execute(find_name_sql, (row['company_name'],))\n",
    "            company = curs.fetchone()\n",
    "    \n",
    "            if company:\n",
    "                company_id = company['company_id']\n",
    "                # 중복 체크\n",
    "                check_sql = \"\"\"\n",
    "                SELECT COUNT(*) FROM Posting WHERE company_id = %s AND title = %s\n",
    "                \"\"\"\n",
    "                curs.execute(check_sql, (company_id, row['title']))\n",
    "                result = curs.fetchone()\n",
    "    \n",
    "                if result['COUNT(*)'] == 0:  # 중복되지 않으면 삽입\n",
    "                    # 공고 정보 삽입\n",
    "                    sql = \"\"\"\n",
    "                    INSERT INTO Posting (company_id, title, career, education, deadline, skill)\n",
    "                    VALUES (%s, %s, %s, %s, %s, %s)\n",
    "                    \"\"\"\n",
    "                    values = (\n",
    "                        company_id,\n",
    "                        row['title'],\n",
    "                        row['career'],\n",
    "                        row['education'],\n",
    "                        row['deadline'],\n",
    "                        row['skill']\n",
    "                    )\n",
    "                    curs.execute(sql, values)\n",
    "                    conn.commit()\n",
    "                else:\n",
    "                    print(f\"중복된 공고 제목: {row['title']}\")\n",
    "            else:\n",
    "                print(f\"회사를 찾을 수 없음: {row['company_name']}\")\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"2. 오류 발생 {e}\")\n",
    "            conn.rollback()\n",
    "\n",
    "\n",
    "    # 대기업 정보 삽입\n",
    "    for index, row in curation_data.iterrows():\n",
    "        try:\n",
    "            # 중복 확인\n",
    "            check_sql = \"\"\"\n",
    "            SELECT COUNT(*) FROM Top WHERE curation_company_name = %s\n",
    "            \"\"\"\n",
    "            curs.execute(check_sql, (row['curation_company_name'],))\n",
    "            result = curs.fetchone()\n",
    "            if result['COUNT(*)'] == 0:  # 중복이 없을 경우\n",
    "                sql = \"\"\"\n",
    "                INSERT INTO Top (curation_company_name, curation_company_type, curation_company_year, curation_company_genre)\n",
    "                VALUES (%s, %s, %s, %s)\n",
    "                \"\"\"\n",
    "                values = (\n",
    "                    row['curation_company_name'],\n",
    "                    row['curation_company_type'],\n",
    "                    row['curation_company_year'],\n",
    "                    row['curation_company_genre']\n",
    "                )\n",
    "                curs.execute(sql, values)\n",
    "                conn.commit()\n",
    "            else:\n",
    "                print()#f\"중복된 대기업: {row['curation_company_name']}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"3. 오류 발생 {e}\")\n",
    "            conn.rollback()\n",
    "\n",
    "except pymysql.MySQLError as e:\n",
    "    print(f\"DB 연결 실패: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "finally:\n",
    "    if 'conn' in locals() and conn.open:\n",
    "        conn.close()\n",
    "        print(\"DB 연결 닫힘.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
